{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebfcb23b-5fe8-44c7-af08-e9c827454040",
   "metadata": {},
   "source": [
    "# Perform bootstrap to estimate stability of networks and network measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc637e3-f121-45d5-898f-ad7de817ae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inverse_covariance import (\n",
    "    QuicGraphicalLasso,\n",
    "    QuicGraphicalLassoCV,\n",
    "    QuicGraphicalLassoEBIC,\n",
    "    AdaptiveGraphicalLasso,\n",
    "    ModelAverage,\n",
    ")\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import tabulate\n",
    "import time\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_sparse_spd_matrix\n",
    "from sklearn.covariance import GraphicalLassoCV, ledoit_wolf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import ast\n",
    "import os.path as op\n",
    "import pickle\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.utils import resample\n",
    "from sklearn.covariance import GraphicalLassoCV\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from itertools import combinations\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial import distance\n",
    "import scipy.stats as stats\n",
    "\n",
    "import networkx as nx\n",
    "import glob, os\n",
    "from functools import partial\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e8855-6fd1-481c-98db-47996b0469ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_model_average_sklearn(\n",
    "    X, \n",
    "    method, penalization='random', \n",
    "    n_trials=100, \n",
    "    metric='log_likelihood', \n",
    "    support_thresh=0.5, \n",
    "    cv=10, \n",
    "    lam=None, \n",
    "    alphas=np.logspace(-5, 1, num=20)\n",
    "):\n",
    "    \"\"\"Run ModelAverage in default mode (QuicGraphicalLassoCV) to obtain proportion\n",
    "    matrix.\n",
    "\n",
    "    NOTE:  Only method = 'binary' really makes sense in this case.\n",
    "    \"\"\"\n",
    "    n_trials = n_trials\n",
    "\n",
    "    if lam is None:\n",
    "        cv_model = GraphicalLassoCV(\n",
    "            alphas=alphas,\n",
    "            cv=cv\n",
    "        )\n",
    "        cv_model.fit(X)\n",
    "        lam = cv_model.alpha_\n",
    "    else:\n",
    "        lam = lam\n",
    "\n",
    "    model = AdaptiveGraphicalLasso(\n",
    "        estimator=ModelAverage(\n",
    "            n_trials=n_trials, \n",
    "            penalization=penalization, \n",
    "            lam=lam, \n",
    "            n_jobs=10, \n",
    "            support_thresh=support_thresh,\n",
    "            # subsample = 0.6,\n",
    "        ),\n",
    "        method=method,\n",
    "    )\n",
    "    model.fit(X)\n",
    "    lam_norm_ = np.linalg.norm(model.estimator_.lam_)\n",
    "    return model.estimator_.precision_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7eb06-4e57-48c0-8510-a0ea575717b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_graph_structure_adaptive_average_sklearn(\n",
    "    df, \n",
    "    n_trials=100, \n",
    "    penalization='random',\n",
    "    score_metric=\"log_likelihood\", \n",
    "    cv=10, \n",
    "    lam=None,\n",
    "    threshold=0.5, \n",
    "    alphas=np.logspace(-5, 1, num=20)\n",
    "):\n",
    "    \n",
    "    # standardize the time series: using correlations rather than covariance\n",
    "    # former is more efficient for structure recovery\n",
    "    X = df.to_numpy()\n",
    "    X -= X.mean(axis=0)\n",
    "    X /= X.std(axis=0)\n",
    "\n",
    "    prec_adaptive = adaptive_model_average_sklearn(\n",
    "        X, \n",
    "        penalization='random',\n",
    "        method='binary', \n",
    "        n_trials=n_trials, \n",
    "        metric=score_metric,\n",
    "        support_thresh=threshold,\n",
    "        cv=cv,\n",
    "        lam=lam,\n",
    "        alphas=alphas,\n",
    "    )\n",
    "    \n",
    "    precision_matrix_df = pd.DataFrame(prec_adaptive, columns = df.columns, index = df.columns)\n",
    "            \n",
    "    return precision_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b2244-c779-4931-aa0b-74d652ee360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_graph(\n",
    "    X, \n",
    "    n_trials=1000, \n",
    "    score_metric=\"log_likelihood\", \n",
    "    cv=3, \n",
    "    threshold=0.65, \n",
    "    alphas=np.linspace(0.01, 0.1, 20)\n",
    "):\n",
    "    precision_matrix_df = learn_graph_structure_adaptive_average_sklearn(\n",
    "            X, \n",
    "            penalization='random',\n",
    "            n_trials=n_trials,\n",
    "            score_metric=score_metric,\n",
    "            cv=cv,\n",
    "            lam=None,\n",
    "            alphas=alphas,\n",
    "            threshold=threshold,\n",
    "    )\n",
    "\n",
    "    this_links = get_links(precision_matrix_df)\n",
    "    this_links['weight'] = this_links['weight'].abs()\n",
    "    G_ = nx.from_pandas_edgelist(this_links,'var1','var2', edge_attr='weight', create_using=nx.Graph())\n",
    "    \n",
    "    return G_, precision_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd6d54-a148-4f25-bd29-7025f45d4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(precision_matrix_df, threshold=0.02):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame of links between nodes based on the provided precision matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    precision_matrix_df : pd.DataFrame\n",
    "        A DataFrame that consists of 3 columns: var1, var2, and weight.\n",
    "    threshold : float\n",
    "        Threshold for inverse covariance estimate to be considered as non-zero. Allows to filter out very weak connections. Default is 0.2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    links : pd.DataFrame\n",
    "        A DataFrame that consists of 3 columns: var1, var2, and weight.\n",
    "    \"\"\"\n",
    "    precision_matrix_df = precision_matrix_df.where(np.triu(np.ones(precision_matrix_df.shape)).astype(bool))\n",
    "\n",
    "    links = precision_matrix_df.stack().reset_index()\n",
    "    links.columns = ['var1', 'var2','weight']\n",
    "    links=links.loc[ (abs(links['weight']) > threshold) &  (links['var1'] != links['var2']) ]\n",
    "    links = links.round(3)\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9ca5f-7a56-46e8-b565-5ed81a6174d3",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ab7cc-2745-492e-b13e-d808504a7125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = train | test\n",
    "dataset = 'train'\n",
    "test = False if dataset == 'train' else True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4a494c-1f5c-4230-ba45-90cbca6409e2",
   "metadata": {},
   "source": [
    "Load train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df998e0-bdbf-49d6-9b1e-eb30320aea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ern_df = pd.read_pickle(f\"new_data/models_pickles_new_dass/ern_models_{dataset}_id_clean.pkl\")\n",
    "results_ern_lat_demo_df = pd.read_pickle(f\"new_data/models_pickles_new_dass/ern_cov_fal_models_{dataset}_id_clean.pkl\")\n",
    "results_crn_df = pd.read_pickle(f\"new_data/models_pickles_new_dass/crn_models_{dataset}_id_clean.pkl\")\n",
    "results_crn_lat_demo_df = pd.read_pickle(f\"new_data/models_pickles_new_dass/crn_cov_fal2_models_{dataset}_id_clean.pkl\")\n",
    "\n",
    "results_ern_crn_df = pd.read_pickle(f\"new_data/models_pickles_new_dass/ern_crn_models_{dataset}_id_clean.pkl\")\n",
    "results_ern_crn_lat_demo_df = pd.read_pickle(f\"new_data/models_pickles_new_dass/ern_crn_cov_fal_models_{dataset}_id_clean.pkl\")\n",
    "\n",
    "train_datasets = [results_ern_df, results_ern_lat_demo_df, results_crn_df, results_crn_lat_demo_df, results_ern_crn_df, results_ern_crn_lat_demo_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71c1e2-b1c4-47c4-8f39-cd3b568555c9",
   "metadata": {},
   "source": [
    "Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9dc297-23ba-42aa-ac01-870c0ccbe15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ern_test_df = pd.read_pickle(f\"new_data/models_pickles_new_dass/ern_models_test_id_clean.pkl\")\n",
    "results_ern_lat_demo_test_df = pd.read_pickle(f\"new_data/models_pickles_new_dass/ern_cov_fal_models_test_id_clean.pkl\")\n",
    "results_crn_test_df = pd.read_pickle(f\"new_data/models_pickles_new_dass/crn_models_test_id_clean.pkl\")\n",
    "results_crn_lat_demo_test_df = pd.read_pickle(f\"new_data/models_pickles_new_dass/crn_cov_fal2_models_test_id_clean.pkl\")\n",
    "\n",
    "results_ern_crn_test_df = pd.read_pickle(f\"new_data/models_pickles_new_dass/ern_crn_models_test_id_clean.pkl\")\n",
    "results_ern_crn_lat_demo_test_df = pd.read_pickle(f\"new_data/models_pickles_new_dass/ern_crn_cov_fal_models_test_id_clean.pkl\")\n",
    "\n",
    "# fill nan\n",
    "results_crn_lat_demo_test_df['e_LT_F2_C'] = results_crn_lat_demo_test_df['e_LT_F2_C'].fillna(results_crn_lat_demo_test_df['e_LT_F2_C'].mean())\n",
    "results_ern_crn_lat_demo_test_df['e_LT_F2_C'] = results_ern_crn_lat_demo_test_df['e_LT_F2_C'].fillna(results_ern_crn_lat_demo_test_df['e_LT_F2_C'].mean())\n",
    "\n",
    "\n",
    "test_datasets = [results_ern_test_df, results_ern_lat_demo_test_df, results_crn_test_df, results_crn_lat_demo_test_df, results_ern_crn_test_df, results_ern_crn_lat_demo_test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d1ea1-c70c-494e-b6e5-9aa254690471",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop_covs = ['WASH', 'NEU', 'Sex', 'id']\n",
    "columns_to_drop = ['WASH', 'NEU',  'id']\n",
    "\n",
    "# train dataset\n",
    "ern_data_df_ = results_ern_df.drop(columns=columns_to_drop)\n",
    "ern_cov_fal_data_df_ = results_ern_lat_demo_df.drop(columns=columns_to_drop_covs)\n",
    "\n",
    "crn_data_df_ = results_crn_df.drop(columns=columns_to_drop)\n",
    "crn_cov_fal2_data_df_ = results_crn_lat_demo_df.drop(columns=columns_to_drop_covs)\n",
    "\n",
    "ern_crn_data_df_ = results_ern_crn_df.drop(columns=columns_to_drop)\n",
    "ern_crn_cov_fal_data_df_ = results_ern_crn_lat_demo_df.drop(columns=columns_to_drop_covs)\n",
    "\n",
    "# test dataset\n",
    "ern_data_df_test_ = results_ern_test_df.drop(columns=columns_to_drop)\n",
    "ern_cov_fal_data_df_test_ = results_ern_lat_demo_test_df.drop(columns=columns_to_drop_covs)\n",
    "\n",
    "crn_data_df_test_ = results_crn_test_df.drop(columns=columns_to_drop)\n",
    "crn_cov_fal2_data_df_test_ = results_crn_lat_demo_test_df.drop(columns=columns_to_drop_covs)\n",
    "\n",
    "ern_crn_data_df_test_ = results_ern_crn_test_df.drop(columns=columns_to_drop)\n",
    "ern_crn_cov_fal_data_df_test_ = results_ern_crn_lat_demo_test_df.drop(columns=columns_to_drop_covs)\n",
    "\n",
    "\n",
    "datasets_train = [ern_data_df_, ern_cov_fal_data_df_, crn_data_df_, crn_cov_fal2_data_df_, ern_crn_data_df_, ern_crn_cov_fal_data_df_]\n",
    "datasets_test = [ern_data_df_test_, ern_cov_fal_data_df_test_, crn_data_df_test_, crn_cov_fal2_data_df_test_, ern_crn_data_df_test_, ern_crn_cov_fal_data_df_test_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db63dd7-69fc-461b-946a-c027f2b7bc17",
   "metadata": {},
   "source": [
    "## Perform full bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f828b80c-3d6f-433b-a2a5-4383fe11e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_network(\n",
    "    X, \n",
    "    model='adaptive_sklearn', \n",
    "    N=100, \n",
    "):\n",
    "    bootstrapped_matrices = []\n",
    "    n = len(X)\n",
    "    for i in range(N):\n",
    "        \n",
    "        # Generate a bootstrap sample\n",
    "        bootstrap_X = resample(X, n_samples=n, replace=True)\n",
    "        print(f'{i} iteration')\n",
    "        _, precision_matrix = estimate_graph(bootstrap_X)\n",
    "        bootstrapped_matrices.append(precision_matrix)\n",
    "            \n",
    "    return bootstrapped_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f023e-6037-40b9-b2fe-b568f0f1b8e7",
   "metadata": {},
   "source": [
    "Perform bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31480903-4da2-40c3-9bb3-eacf8295c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "for index, dataset in enumerate(datasets_train):\n",
    "    print(f'Estimating {index} dataset ######################')\n",
    "\n",
    "    bootstraped_precision_matrices = bootstrap_network(\n",
    "        X = dataset,\n",
    "        N=1000,\n",
    "    )\n",
    "\n",
    "    with open(f'new_data/bootstrap_results/bootstrap_precision_matrices_{index}.pkl', 'wb') as f:\n",
    "        pickle.dump(bootstraped_precision_matrices, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c74453-bb8b-41ef-98df-367133a9dcfa",
   "metadata": {},
   "source": [
    "## Estimate the stability of network measures using bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5f866-cb35-45ad-975b-90afe3b494de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nodes_predictability(X, G):\n",
    "    explained_variance = dict()\n",
    "        \n",
    "    for node in G.nodes():\n",
    "        y_ = X[[node]]\n",
    "\n",
    "        neighbors = list(G.neighbors(node))\n",
    "\n",
    "        X_ = X.loc[:, neighbors]\n",
    "\n",
    "        lm = LinearRegression()\n",
    "        lm.fit(X_, y_)\n",
    "\n",
    "        score = lm.score(X_,y_)\n",
    "        explained_variance[node] = score\n",
    "\n",
    "    return explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768afa7-c9d4-456a-86f1-80f684b07049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(precision_matrix_df, threshold=0.02):\n",
    "    precision_matrix_df = precision_matrix_df.where(np.triu(np.ones(precision_matrix_df.shape)).astype(bool))\n",
    "    \n",
    "    links = precision_matrix_df.stack().reset_index()\n",
    "    links.columns = ['var1', 'var2','weight']\n",
    "    links=links.loc[ (abs(links['weight']) > threshold) &  (links['var1'] != links['var2']) ]\n",
    "        \n",
    "    links = links.round(3)\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3325abcc-f591-4e50-a827-d18b47edd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_dict(dict_):\n",
    "    items = [(key, value) for key, value in dict_.items()]\n",
    "    sorted_items = sorted(items, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    ranked_dict = {}\n",
    "    rank = 1\n",
    "\n",
    "    for key, value in sorted_items:\n",
    "        ranked_dict[key] = rank\n",
    "        rank += 1\n",
    "\n",
    "    return ranked_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d33686-a34f-4243-8d34-c76671504ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_network_measures(\n",
    "    X, \n",
    "    model='adaptive_sklearn', \n",
    "    measures=None, \n",
    "    N=1000, \n",
    "    levels=np.arange(.95, 0.25, -0.05)\n",
    "):\n",
    "    results_df = pd.DataFrame()\n",
    "    network_measures_baseline = []\n",
    "    \n",
    "    G_, _ = estimate_graph(X)\n",
    "    \n",
    "    for measure, measure_parameters in measures: \n",
    "        network_measure = measure(G = G_, **measure_parameters)\n",
    "        ranked_network_measure = get_ranked_dict(network_measure)\n",
    "        ranked_network_measure = {k: v for k, v in sorted(ranked_network_measure.items(), key=lambda item: item[0], reverse=True)}\n",
    "        logging.info(ranked_network_measure)\n",
    "        baseline_measure = list(ranked_network_measure.values())\n",
    "        \n",
    "        network_measures_baseline.append(baseline_measure)\n",
    "        logging.info('Baseline measures appended')\n",
    "    \n",
    "    for level in levels:\n",
    "        n = int(level * len(X))\n",
    "        print(f'X len before samples: {n}')\n",
    "\n",
    "        for i in range(N):\n",
    "            # Generate a bootstrap sample with replacements\n",
    "            bootstrap_X = resample(X, n_samples=n, replace=True)\n",
    "            print(f'X len inside samples: {len(bootstrap_X)}')\n",
    "            G_, _ = estimate_graph(bootstrap_X)\n",
    "            \n",
    "            for index, network_measure in enumerate(measures): \n",
    "                measure, measure_parameters = network_measure\n",
    "                \n",
    "                current_baseline = network_measures_baseline[index]\n",
    "                \n",
    "                try:\n",
    "                    network_measure = measure(G = G_, **measure_parameters)\n",
    "                    ranked_network_measure = get_ranked_dict(network_measure)\n",
    "                    ranked_network_measure = {k: v for k, v in sorted(ranked_network_measure.items(), key=lambda item: item[0], reverse=True)}\n",
    "                    logging.info(ranked_network_measure)\n",
    "                    ranked_network_measure = list(ranked_network_measure.values())\n",
    "\n",
    "                    try:\n",
    "                        similarity_corr_coef, p_value = scipy.stats.pearsonr(current_baseline, ranked_network_measure)\n",
    "                        print(f\"Measure: {measure.__name__}  Level: {level}   sample: {i}   : similarity: {similarity_corr_coef}\")\n",
    "                        this_results = pd.DataFrame({\n",
    "                            'measure': [measure.__name__],\n",
    "                            'level': [level],\n",
    "                            'similatity': [similarity_corr_coef],\n",
    "                        })\n",
    "\n",
    "                        results_df = pd.concat([results_df, this_results], ignore_index = True)\n",
    "                    except:\n",
    "                        logging.info('DIFFERENT LENGTHS OF BASELINE AND CURRENT')\n",
    "                except:\n",
    "                    logging.info('NETWORK MEASURE ERROR')\n",
    "                    \n",
    "            \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c188e21-de92-4f75-a3d4-fbe1eccf3cbb",
   "metadata": {},
   "source": [
    "Estimate the stability of network measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa051369-907f-4fa0-9059-39652fad4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "for index, (dataset_train, dataset_test) in enumerate(zip(datasets, test_datasets)):\n",
    "\n",
    "    df_ranked = bootstrap_network_measures(\n",
    "        X = dataset_train,\n",
    "        measures = [\n",
    "            (calculate_nodes_predictability, {'X': dataset_test}), \n",
    "            (nx.degree_centrality, {}),\n",
    "            (nx.closeness_centrality, {}),\n",
    "            (nx.current_flow_closeness_centrality, {'weight': 'weight'}),\n",
    "            (nx.betweenness_centrality, {'weight': 'weight'}),\n",
    "            (nx.current_flow_betweenness_centrality, {'weight': 'weight'}),\n",
    "            (nx.load_centrality, {})\n",
    "        ],\n",
    "        N=2,\n",
    "        levels=np.arange(.95, 0.25, -0.05)\n",
    "\n",
    "    )\n",
    "    # df_ranked.to_pickle(f'data/network_analysis/stability_estimates/network_measures_ranked_bootstrapped_{index}_pred_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e3049-a535-43a4-b0b1-2a00c193f4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skggmtest",
   "language": "python",
   "name": "skggmtest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
